{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Is-Coffee-Wet_Tunner.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNNUCzgEtY5YPJs8P0RGhE5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndVFokCDpWGu"
   },
   "source": [
    "# Neural Network Tuner\n",
    "\n",
    "This notebook was made for tuning hyperparameters in a neural network. This is posible by using the library of `keras-tuner`; it runs multiple models for a set of configurations and chooses the best one depending on the performance metric selected.\n",
    "\n",
    "We run the tuner in Google Colab due to how resource intensive the process is; and, to do so, we use this notebook. It contains all the objects and functions necessary to instantiate a neural network model similar to the ones used in the project. Some objects/functions have been simplify to the original as we don't need some parameters that they have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1uPSkF5Zle0"
   },
   "source": [
    "## Install & import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZUpdM92j24W"
   },
   "source": [
    "First, install the `keras-tuner` in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ixZUQHL2VoL3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1603052759565,
     "user_tz": 360,
     "elapsed": 3615,
     "user": {
      "displayName": "Andrés Ramírez Quirós",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4VLfFJdiW15q82DftUdzLKkxb81UXWEJjsfImw3c=s64",
      "userId": "00417801794163047776"
     }
    },
    "outputId": "881726c2-e2ed-407b-ef01-0b85cbcb1dbb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    }
   },
   "source": [
    "!pip install -U keras-tuner"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras-tuner in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: terminaltables in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.16.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBl-hsoNl-fs"
   },
   "source": [
    "Import the libraries needed to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ksUHSS4sOPqa"
   },
   "source": [
    "import io                                       # Read and writing files in the notebook session\n",
    "import matplotlib.pyplot as plt                 # Graphs used in WindowGenerator\n",
    "import numpy as np                              # Numpy\n",
    "import pandas as pd                             # Load datasets from csv\n",
    "import kerastuner as kt                         # Tunner of the models\n",
    "import tensorflow as tf                         # Tensorflow\n",
    "import tensorflow.keras.layers as layers        # Easier way to call layers instances\n",
    "import tensorflow_addons.layers as layers_addon # Used in WeightNormalization\n",
    "\n",
    "from google.colab import files                  # To upload and download files from the colab session\n",
    "import IPython                                  # To clean the output of a cell"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlRPtHW6ZTmH"
   },
   "source": [
    "## Upload files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yfw2z1q6ZQp4"
   },
   "source": [
    "uploaded = files.upload()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqYHXu79rcl4"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIIK6WSAm_r9"
   },
   "source": [
    "### ConfigFile\n",
    "\n",
    "The `ConfigFile` object used for splitting the datasets into training, validation and test. More information about how to use it is available in the original project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8BNiLLI5OoCJ"
   },
   "source": [
    "class ConfigFile:\n",
    "    def __init__(self):\n",
    "        self.num_data = 0\n",
    "        self.training = 1\n",
    "        self.validation = 0.1\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2mdRno9n26p"
   },
   "source": [
    "### Standardization\n",
    "\n",
    "Apply a standardization to all the the dataset by using the z-score. We assume the weather data is stationary to apply this function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PZ61RAAJP2jf"
   },
   "source": [
    "def standardize(dataset):\n",
    "    # Computes the mean and standard deviation\n",
    "    ds_mean = dataset.mean()\n",
    "    ds_std = dataset.std()\n",
    "\n",
    "    # Apply the method to the dataset\n",
    "    dataset = (dataset - ds_mean) / ds_std\n",
    "\n",
    "    return dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfp3aimeoxFl"
   },
   "source": [
    "### Split dataset\n",
    "\n",
    "Split the dataset into the datetime index originally used, training set, validation set and test set. The amount is data to use for each is determined by the `config_file`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p-AwjfnEJagb"
   },
   "source": [
    "def split_dataset(dataset, config_file):\n",
    "    # Resets index to add datetime as a normal column\n",
    "    dataset = dataset.reset_index()\n",
    "\n",
    "    # Pops the datetime from the dataset. Not use in the NN explicitly\n",
    "    datetime_index = dataset.pop(\"Datetime\")\n",
    "\n",
    "    # Accumulates the ratio to use in slices.\n",
    "    # Validation set is taken from the training set.\n",
    "    train_ratio = config_file.training - config_file.validation  # e.g. from 0 to 0.5\n",
    "    validation_ratio = config_file.training  # e.g. from 0.5 to 0.7\n",
    "\n",
    "    # Divides the dataset\n",
    "    train_ds = dataset[0:int(config_file.num_data * train_ratio)]\n",
    "    val_ds = dataset[int(config_file.num_data * train_ratio):\n",
    "                     int(config_file.num_data * validation_ratio)]\n",
    "    test_ds = dataset[int(config_file.num_data * validation_ratio):]\n",
    "\n",
    "    return datetime_index, train_ds, val_ds, test_ds"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKdZE6tOnSe5"
   },
   "source": [
    "### WindowGenerator\n",
    "\n",
    "The `WindowGenerator` used to set the input and label lenght, saves the splitted dataset and convert them to a `tf.dataset` when called by its property."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tSWvHlGpPqOf"
   },
   "source": [
    "class WindowGenerator:\n",
    "\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_ds=None, val_ds=None, test_ds=None,\n",
    "                 label_columns=None, batch_size=128):\n",
    "        # Store the raw data (whatever type it is, but generally DataFrame).\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.test_ds = test_ds\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        # Associate each label column with a number to use as internal reference\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                          enumerate(label_columns)}\n",
    "\n",
    "        # Do the same with the input columns for the network\n",
    "        # Remember that not all columns in the input are predicted (only labels)\n",
    "        if train_ds is not None:\n",
    "            self.column_indices = {name: i for i, name in\n",
    "                                   enumerate(train_ds.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        # Size of the batches when creating the tensorflow.data.Dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Calculates the total amount of time-steps the window will take\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        # Slices used to travel the dataset\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        # The label (prediction) will always start counting from the end\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "\n",
    "        # Slices used to travel the dataset\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]]\n",
    "                 for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def plot(self, plot_col, model=None, max_subplots=3):\n",
    "        # Takes the inputs and labels from the example\n",
    "        inputs, labels = self.example\n",
    "\n",
    "        # Sets the figure size\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Gets the number of the index to plot from the input dictionary\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "\n",
    "        # Resolves the amount of plots to do\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "\n",
    "        # Plots the points in the graph\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(3, 1, n + 1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            # Gets the number of the index to plot from the label dictionary\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(\n",
    "                    plot_col, None)\n",
    "\n",
    "            # If column isn't a label, then no labels are show in the graph\n",
    "            # Use the plot_col as only the input\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            # Don't graph the labels and continue with the next iteration\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            # Graphs the label points\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "\n",
    "            # Generates the predictions to graph\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                            marker='X', edgecolors='k', label='Predictions',\n",
    "                            c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "        plt.show()\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "            batch_size=self.batch_size, )\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_ds)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_ds)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_ds)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"\n",
    "        Get and cache an example batch of `inputs, labels` for plotting\n",
    "        from the training set.\n",
    "        \"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "\n",
    "        return result\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr0u0cy4pFbO"
   },
   "source": [
    "### Gated activation\n",
    "\n",
    "Gated activation used in the `ResidualBlock` layers and the `Conv1D` layer to preserve the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "46x679NGOK0Q"
   },
   "source": [
    "# Taken from https://github.com/xadrianzetx/temporal-conv-net-keras/blob/master/tcnet/activations.py\n",
    "\n",
    "def gated_activation(x):\n",
    "    # Used in PixelCNN and WaveNet\n",
    "    tanh = layers.Activation('tanh')(x)\n",
    "    sigmoid = layers.Activation('sigmoid')(x)\n",
    "    return layers.multiply([tanh, sigmoid])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1zlgrIlpY_a"
   },
   "source": [
    "### ResidualBlock\n",
    "\n",
    "Implementation of the residual layer used in a temporal convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4hUq4HicOdkh"
   },
   "source": [
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=4, stride=1, dilation=1,\n",
    "                 padding=\"causal\", dropout=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = layers_addon.WeightNormalization(layers.Conv1D(filters=filters,\n",
    "                                                                    kernel_size=kernel_size,\n",
    "                                                                    strides=stride,\n",
    "                                                                    padding=padding,\n",
    "                                                                    dilation_rate=dilation))\n",
    "        self.activation1 = layers.Activation(\"relu\")\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = layers_addon.WeightNormalization(layers.Conv1D(filters=filters,\n",
    "                                                                    kernel_size=kernel_size,\n",
    "                                                                    strides=stride,\n",
    "                                                                    padding=padding,\n",
    "                                                                    dilation_rate=dilation))\n",
    "        self.activation2 = layers.Activation(\"relu\")\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "        # Residual layer of conv 1x1 for feature mapping\n",
    "        self.residual = layers.Conv1D(filters=filters,\n",
    "                                      kernel_size=1)\n",
    "        \n",
    "        # Layer to add the residual layer with the rest\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.activation1(x)\n",
    "        if training:\n",
    "            x = self.dropout1(x, training=training)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        if training:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        # Adds the residual layer\n",
    "        y = self.residual(inputs)\n",
    "        x = self.add([x, y])\n",
    "\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHrMjQzOrjWi"
   },
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-Bj8fmJpiix"
   },
   "source": [
    "Uploads the dataset to used in the training and print a description of it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h9i67BKyQov4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1603053021145,
     "user_tz": 360,
     "elapsed": 1033,
     "user": {
      "displayName": "Andrés Ramírez Quirós",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4VLfFJdiW15q82DftUdzLKkxb81UXWEJjsfImw3c=s64",
      "userId": "00417801794163047776"
     }
    },
    "outputId": "95f8daec-2ecf-4988-d3f9-576fc6b107f9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    }
   },
   "source": [
    "# Loads the dataset\n",
    "dataset =  pd.read_csv('benchmark_hour.csv',\n",
    "                      engine=\"c\", index_col=\"Datetime\", parse_dates=True)\n",
    "\n",
    "# Information of the dataset\n",
    "print(dataset.info(verbose=True))\n",
    "print(dataset.describe().transpose())"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 90393 entries, 2010-04-07 00:00:00 to 2020-07-29 08:00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Temp Out        90393 non-null  float64\n",
      " 1   Hi Temp         90393 non-null  float64\n",
      " 2   Low Temp        90393 non-null  float64\n",
      " 3   Out Hum         90393 non-null  float64\n",
      " 4   Wind Speed      90393 non-null  float64\n",
      " 5   Hi Speed        90393 non-null  float64\n",
      " 6   Bar             90393 non-null  float64\n",
      " 7   Rain            90393 non-null  float64\n",
      " 8   Solar Rad.      90393 non-null  float64\n",
      " 9   Hi Solar Rad.   90393 non-null  float64\n",
      " 10  In Temp         90393 non-null  float64\n",
      " 11  In Hum          90393 non-null  float64\n",
      " 12  Soils 1 Moist.  90393 non-null  float64\n",
      " 13  Leaf Wet 1      90393 non-null  float64\n",
      " 14  Leaf Wet Accum  90393 non-null  float64\n",
      " 15  day sin         90393 non-null  float64\n",
      " 16  day cos         90393 non-null  float64\n",
      " 17  year sin        90393 non-null  float64\n",
      " 18  year cos        90393 non-null  float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 13.8 MB\n",
      "None\n",
      "                  count        mean  ...         75%      max\n",
      "Temp Out        90393.0   20.914795  ...   23.575000    33.90\n",
      "Hi Temp         90393.0   21.549301  ...   24.600000    35.30\n",
      "Low Temp        90393.0   20.299488  ...   22.600000    32.30\n",
      "Out Hum         90393.0   79.956534  ...   93.000000    99.00\n",
      "Wind Speed      90393.0    1.823435  ...    2.800000    23.70\n",
      "Hi Speed        90393.0   10.826820  ...   17.700000    61.20\n",
      "Bar             90393.0  759.966270  ...  782.075000  1045.30\n",
      "Rain            90393.0    0.263969  ...    0.000000    75.80\n",
      "Solar Rad.      90393.0  194.247774  ...  339.000000  1228.00\n",
      "Hi Solar Rad.   90393.0  282.364442  ...  548.000000  1570.00\n",
      "In Temp         90393.0   23.061830  ...   23.825000    40.05\n",
      "In Hum          90393.0   62.300366  ...   69.000000    94.00\n",
      "Soils 1 Moist.  90393.0  120.534201  ...  200.000000   200.00\n",
      "Leaf Wet 1      90393.0    5.292777  ...   15.000000    15.00\n",
      "Leaf Wet Accum  90393.0   25.286980  ...   60.000000    60.00\n",
      "day sin         90393.0    0.000068  ...    0.707107     1.00\n",
      "day cos         90393.0    0.000039  ...    0.707107     1.00\n",
      "year sin        90393.0    0.012408  ...    0.714123     1.00\n",
      "year cos        90393.0   -0.022435  ...    0.689566     1.00\n",
      "\n",
      "[19 rows x 8 columns]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9QBhJXTpqtX"
   },
   "source": [
    "Prepares the objects used in the training. This means creating the config file to split the data, standardizing the dataset and dividing it, and creating the window used in the training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JvfYq7ohQRjZ"
   },
   "source": [
    "# *** Config File\n",
    "# Use in the dataset partition\n",
    "config = ConfigFile()\n",
    "\n",
    "# Use the entire data with the benchmarks, as the models won't be saved\n",
    "config.num_data, config.num_features = dataset.shape\n",
    "\n",
    "# *** Dataset preparation\n",
    "# Normalize the dataset\n",
    "dataset = standardize(dataset)\n",
    "\n",
    "# Partition the dataset\n",
    "_, train, val, test = split_dataset(dataset, config)\n",
    "\n",
    "# *** Window\n",
    "# A week in hours\n",
    "input_width = 7 * 24\n",
    "label_width = input_width\n",
    "label_columns = dataset.columns.tolist()\n",
    "\n",
    "# Removes th sin/cos columns from the labels\n",
    "label_columns = label_columns[:-4]\n",
    "\n",
    "# Window of 7 days for testing the NN\n",
    "window = WindowGenerator(input_width=input_width,\n",
    "                         label_width=label_width,\n",
    "                         shift=label_width,\n",
    "                         train_ds=train,\n",
    "                         val_ds=val,\n",
    "                         test_ds=test,\n",
    "                         label_columns=label_columns,\n",
    "                         batch_size=256)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoZmICGdrp62"
   },
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj3IzUL26Bp8"
   },
   "source": [
    "### Convolutional model\n",
    "\n",
    "Function that return a model of a convolutional network with the hyperparamaters tuner placed in the desired parameters.\n",
    "\n",
    "The hyperparameters tuned are:\n",
    "\n",
    "- Filter size for each of a 3-layer architecture\n",
    "  - From 16 to 512 using steps of 32\n",
    "- Learning rate\n",
    "  - 0.01, 0.001 or 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3xyoATD_sD-v"
   },
   "source": [
    "def convolutional_builder(hp):\n",
    "    # Shape => [batch, input_width, features]\n",
    "    inputs = layers.Input(shape=(168, 19))\n",
    "\n",
    "    # Tune the number of filters in the first convolutional layer\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=16, max_value=512, step=32)\n",
    "    x = layers.Conv1D(filters=hp_filters_1,\n",
    "                      kernel_size=24,\n",
    "                      activation=\"relu\")(inputs)\n",
    "\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.MaxPool1D(pool_size=2)(x)\n",
    "\n",
    "    # Tune the number of filters in the second convolutional layer\n",
    "    hp_filters_2 = hp.Int('filters_2', min_value=16, max_value=512, step=32)\n",
    "    x = layers.Conv1D(filters=hp_filters_2,\n",
    "                      kernel_size=24,\n",
    "                      activation=\"relu\")(x)\n",
    "\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.MaxPool1D(pool_size=2)(x)\n",
    "\n",
    "    # Tune the number of filters in the third convolutional layer\n",
    "    hp_filters_3 = hp.Int('filters_3', min_value=16, max_value=512, step=32)\n",
    "    x = layers.Conv1D(filters=hp_filters_3,\n",
    "                      kernel_size=24,\n",
    "                      activation=\"relu\")(x)\n",
    "\n",
    "    # Shape => [batch, 1,  label_width*label_columns]\n",
    "    dense = layers.Dense(units=168*15,\n",
    "                         activation=\"linear\")(x)\n",
    "\n",
    "    # Shape => [batch, label_width, label_columns]\n",
    "    outputs = layers.Reshape([168, 15])(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"conv_model\")\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError(),\n",
    "                           tf.metrics.MeanAbsolutePercentageError()])\n",
    "\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T22XG7WiqN-4"
   },
   "source": [
    "### Temporal convolutional model\n",
    "\n",
    "Function that return a model of a temporal convolutional network with the hyperparamaters tuner placed in the desired parameters.\n",
    "\n",
    "The hyperparameters tuned are:\n",
    "\n",
    "- Filter size for each layer\n",
    "  - From 32 to 256 using steps of 32\n",
    "- Kernel size for each layer\n",
    "  - From 2 to 12 using steps of 2\n",
    "- Amount of dilations (influence the depth of the NN)\n",
    "  - From 1 to 6 using steps of 2\n",
    "- Learning rate\n",
    "  - 0.01, 0.001 or 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pdBunD6uI2r-"
   },
   "source": [
    "def temporal_builder(hp):\n",
    "    # Shape => [batch, input_width, features]\n",
    "    inputs = layers.Input(shape=(168, 19))\n",
    "\n",
    "    # Tune the number of filters in the first convolutional layer\n",
    "    hp_filters_0 = hp.Int('filters_0', min_value=32, max_value=256, step=32)\n",
    "    hp_kernels_0 = hp.Int('kernels_0', min_value=2, max_value=12, step=2)\n",
    "    x = ResidualBlock(filters=hp_filters_0, kernel_size=hp_kernels_0)(inputs)\n",
    "\n",
    "    # Tune the number of dilations that the network has\n",
    "    dilations = hp.Int('dilations', min_value=1, max_value=6, step=2)\n",
    "\n",
    "    for factor in range(1, dilations):\n",
    "        # Tune the number of filters in the n-th convolutional layer\n",
    "        hp_filters = hp.Int('filters_{}'.format(factor), min_value=32, max_value=256, step=32)\n",
    "        hp_kernels = hp.Int('kernels_{}'.format(factor), min_value=2, max_value=12, step=2)\n",
    "\n",
    "        dilation = 2 ** factor\n",
    "        x = ResidualBlock(filters=hp_filters, kernel_size=hp_kernels,\n",
    "                              dilation=dilation)(x)\n",
    "\n",
    "    # Shape => [batch, label_width, label_columns]\n",
    "    output = layers.Dense(15)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name=\"tcn_model\")\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError(),\n",
    "                           tf.metrics.MeanAbsolutePercentageError()])\n",
    "\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ov0sFQYrwMc"
   },
   "source": [
    "## Instantiate the tuner and perform hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl8qZ3tqq4ti"
   },
   "source": [
    "Callback to clear the cell's output. Prevents a cluster of information during the tuning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "itM468s8S8De"
   },
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcVXGg8VeDsE"
   },
   "source": [
    "### Convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFm_lG1aeEAm"
   },
   "source": [
    "Instantiate a tuner object with the model to use, the objective to maximize/minimize, and other variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7zXtCeXv6obF"
   },
   "source": [
    "convolutional_tuner = kt.Hyperband(convolutional_builder,\n",
    "                                   objective=\"val_loss\",\n",
    "                                   max_epochs=10,\n",
    "                                   factor=3,\n",
    "                                   directory=\"my_dir\",\n",
    "                                   project_name=\"tuner_convolutional\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V48ajprPrBmE"
   },
   "source": [
    "Use the tuner to search for the best values. Then print the best model configuration and its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xTOkDbnkeQIT"
   },
   "source": [
    "convolutional_tuner.search(window.train, epochs=10, validation_data=window.val, callbacks = [ClearTrainingOutput()])\n",
    "\n",
    "# Cleans the output\n",
    "IPython.display.clear_output(wait = True)\n",
    "\n",
    "# Get the optimal hyperparameters and model\n",
    "best_conv_hps = convolutional_tuner.get_best_hyperparameters(3)\n",
    "best_conv_model = convolutional_tuner.get_best_models(3)\n",
    "\n",
    "#Prints the best three models\n",
    "for i in range(3):\n",
    "  print(\"The hyperparameter search is complete. The optimal model #{} is:\".format(i))\n",
    "  print(\"Learning rate: {}\".format(best_conv_hps[i].get('learning_rate')))\n",
    "\n",
    "  print(\"\\nSummary of the convolutional layers:\")\n",
    "  conv_layer_counter = 0\n",
    "  for model_layers in best_conv_model[i].layers:\n",
    "    # Print the values of the convolutional layer\n",
    "    if isinstance(model_layers, layers.Conv1D):\n",
    "      print(\"Layer #{}. Filters: {}\".format(conv_layer_counter, \n",
    "                                            model_layers.filters))\n",
    "      conv_layer_counter += 1\n",
    "\n",
    "  print(best_conv_model[i].summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22YSWyPxdxdZ"
   },
   "source": [
    "### Temporal convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVdKMfccqoCd"
   },
   "source": [
    "Instantiate a tuner object with the model to use, the objective to maximize/minimize, and other variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WUJBSsL1IvUc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1603054430425,
     "user_tz": 360,
     "elapsed": 1065,
     "user": {
      "displayName": "Andrés Ramírez Quirós",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4VLfFJdiW15q82DftUdzLKkxb81UXWEJjsfImw3c=s64",
      "userId": "00417801794163047776"
     }
    },
    "outputId": "ddc53c2a-c222-4d54-8b07-3229ce434675",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "temporal_tuner = kt.Hyperband(temporal_builder,\n",
    "                              objective=\"val_loss\",\n",
    "                              max_epochs=10,\n",
    "                              factor=3,\n",
    "                              seed=5,\n",
    "                              directory=\"my_dir\",\n",
    "                              project_name=\"tuner_temporal\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project my_dir/tuner_temporal/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from my_dir/tuner_temporal/tuner0.json\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfklRIWTfT4n"
   },
   "source": [
    "Use the tuner to search for the best values. Then print the best model configuration and its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mEVpLJ6LISej",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1603059643511,
     "user_tz": 360,
     "elapsed": 5214147,
     "user": {
      "displayName": "Andrés Ramírez Quirós",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4VLfFJdiW15q82DftUdzLKkxb81UXWEJjsfImw3c=s64",
      "userId": "00417801794163047776"
     }
    },
    "outputId": "5d3c6bd4-c409-4c5f-afd8-400fcc54442e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "temporal_tuner.search(window.train, epochs=10, validation_data=window.val, callbacks = [ClearTrainingOutput()])\n",
    "\n",
    "# Cleans the output\n",
    "IPython.display.clear_output(wait = True)\n",
    "\n",
    "# Get the optimal hyperparameters and model\n",
    "best_temp_hps = temporal_tuner.get_best_hyperparameters(3)\n",
    "best_temp_model = temporal_tuner.get_best_models(3)\n",
    "\n",
    "#Prints the best three models\n",
    "for i in range(3):\n",
    "  print(\"The hyperparameter search is complete. The optimal model #{} is:\".format(i))\n",
    "  print(\"Learning rate: {}\".format(best_temp_hps[i].get('learning_rate')))\n",
    "  print(\"Dilations: {}\".format(best_temp_hps[i].get('dilations')))\n",
    "\n",
    "  print(\"\\nSummary of the residual layers:\")\n",
    "  residual_layer_counter = 0\n",
    "  for model_layers in best_temp_model[i].layers:\n",
    "    # Print the values of the Residual Layer\n",
    "    if isinstance(model_layers, ResidualBlock):\n",
    "      # Unwraps the weight normalization layer into the Conv1D\n",
    "      print(\"Layer #{}. Filters: {}\".format(residual_layer_counter, \n",
    "                                            model_layers.layers[0].layer.filters))\n",
    "      print(\"Layer #{}. Kernels: {}\".format(residual_layer_counter, \n",
    "                                            model_layers.layers[0].layer.kernel_size))\n",
    "      print(\"Layer #{}. Dilation: {}\".format(residual_layer_counter, \n",
    "                                            model_layers.layers[0].layer.dilation_rate))\n",
    "      residual_layer_counter += 1\n",
    "\n",
    "  print(best_temp_model[i].summary())"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 3366e6ff7700b81814f0882e2fee8a52</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.4343114495277405</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dilations: 5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-filters_0: 192</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-filters_1: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-filters_2: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-filters_3: 96</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-filters_4: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-kernels_0: 6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-kernels_1: 12</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-kernels_2: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-kernels_3: 6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-kernels_4: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "The hyperparameter search is complete. The optimal model is:\n",
      "Learning rate: 0.0001\n",
      "Dilations: 5\n",
      "\n",
      "Summary of the residual layers:\n",
      "Layer #0. Filters: 192\n",
      "Layer #0. Kernels: (6,)\n",
      "Layer #0. Dilation: (1,)\n",
      "Layer #1. Filters: 64\n",
      "Layer #1. Kernels: (12,)\n",
      "Layer #1. Dilation: (2,)\n",
      "Layer #2. Filters: 128\n",
      "Layer #2. Kernels: (10,)\n",
      "Layer #2. Dilation: (4,)\n",
      "Layer #3. Filters: 96\n",
      "Layer #3. Kernels: (6,)\n",
      "Layer #3. Dilation: (8,)\n",
      "Layer #4. Filters: 128\n",
      "Layer #4. Kernels: (10,)\n",
      "Layer #4. Dilation: (16,)\n",
      "Model: \"tcn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 168, 19)]         0         \n",
      "_________________________________________________________________\n",
      "residual_block (ResidualBloc (None, 168, 192)          491138    \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl (None, 168, 64)           405954    \n",
      "_________________________________________________________________\n",
      "residual_block_2 (ResidualBl (None, 168, 128)          500610    \n",
      "_________________________________________________________________\n",
      "residual_block_3 (ResidualBl (None, 168, 96)           271010    \n",
      "_________________________________________________________________\n",
      "residual_block_4 (ResidualBl (None, 168, 128)          586626    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 168, 15)           1935      \n",
      "=================================================================\n",
      "Total params: 2,257,273\n",
      "Trainable params: 1,154,863\n",
      "Non-trainable params: 1,102,410\n",
      "_________________________________________________________________\n",
      "None\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JUsvLl9S23g7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1603060637535,
     "user_tz": 360,
     "elapsed": 3055,
     "user": {
      "displayName": "Andrés Ramírez Quirós",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4VLfFJdiW15q82DftUdzLKkxb81UXWEJjsfImw3c=s64",
      "userId": "00417801794163047776"
     }
    },
    "outputId": "017f62b6-8b3f-42d8-b3f5-6e0d6bf762c1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "\n",
      "Summary of the residual layers:\n",
      "Layer #0. Filters: 160\n",
      "Layer #0. Kernels: (10,)\n",
      "Layer #0. Dilation: (1,)\n",
      "Layer #1. Filters: 160\n",
      "Layer #1. Kernels: (2,)\n",
      "Layer #1. Dilation: (2,)\n",
      "Layer #2. Filters: 96\n",
      "Layer #2. Kernels: (2,)\n",
      "Layer #2. Dilation: (4,)\n",
      "Model: \"tcn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 168, 19)]         0         \n",
      "_________________________________________________________________\n",
      "residual_block (ResidualBloc (None, 168, 160)          576962    \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl (None, 168, 160)          231522    \n",
      "_________________________________________________________________\n",
      "residual_block_2 (ResidualBl (None, 168, 96)           114338    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 168, 15)           1455      \n",
      "=================================================================\n",
      "Total params: 924,277\n",
      "Trainable params: 485,487\n",
      "Non-trainable params: 438,790\n",
      "_________________________________________________________________\n",
      "None\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ZtEkBm23GFR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1603060714713,
     "user_tz": 360,
     "elapsed": 8062,
     "user": {
      "displayName": "Andrés Ramírez Quirós",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh4VLfFJdiW15q82DftUdzLKkxb81UXWEJjsfImw3c=s64",
      "userId": "00417801794163047776"
     }
    },
    "outputId": "9cdffe1f-9023-4a73-cf56-4c624af12dfe",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "\n",
      "Summary of the residual layers:\n",
      "Layer #0. Filters: 256\n",
      "Layer #0. Kernels: (6,)\n",
      "Layer #0. Dilation: (1,)\n",
      "Layer #1. Filters: 96\n",
      "Layer #1. Kernels: (10,)\n",
      "Layer #1. Dilation: (2,)\n",
      "Layer #2. Filters: 96\n",
      "Layer #2. Kernels: (2,)\n",
      "Layer #2. Dilation: (4,)\n",
      "Layer #3. Filters: 192\n",
      "Layer #3. Kernels: (6,)\n",
      "Layer #3. Dilation: (8,)\n",
      "Layer #4. Filters: 128\n",
      "Layer #4. Kernels: (12,)\n",
      "Layer #4. Dilation: (16,)\n",
      "Model: \"tcn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 168, 19)]         0         \n",
      "_________________________________________________________________\n",
      "residual_block (ResidualBloc (None, 168, 256)          851458    \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl (None, 168, 96)           701090    \n",
      "_________________________________________________________________\n",
      "residual_block_2 (ResidualBl (None, 168, 96)           83618     \n",
      "_________________________________________________________________\n",
      "residual_block_3 (ResidualBl (None, 168, 192)          683330    \n",
      "_________________________________________________________________\n",
      "residual_block_4 (ResidualBl (None, 168, 128)          1008514   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 168, 15)           1935      \n",
      "=================================================================\n",
      "Total params: 3,329,945\n",
      "Trainable params: 1,707,919\n",
      "Non-trainable params: 1,622,026\n",
      "_________________________________________________________________\n",
      "None\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GJ3-ayqr1GF"
   },
   "source": [
    "## Download results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVND566drMRl"
   },
   "source": [
    "Creates a zip of the tests done and downloads it to the PC."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Ag9leVQ-4qU"
   },
   "source": [
    "# Download log of the temporal test\n",
    "!zip -r /content/temporal.zip /content/my_dir/tuner_temporal/\n",
    "files.download(\"/content/temporal.zip\") "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ni7f2DgugDIB"
   },
   "source": [
    "# Download log of the convolutional test\n",
    "!zip -r /content/convolutional.zip /content/my_dir/tuner_convolutional/\n",
    "files.download( \"/content/convolutional.zip\" ) "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}