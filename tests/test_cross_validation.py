import os
import unittest

import pandas as pd

from benchmarks import cross_validation as cv

PATH_RESOURCES = os.getcwd() + "/resources"


class Test_CrossValidation(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        # Name of the columns in the dataset
        cls.column_names = ["Number", "Loss", "MAE", "MAPE", "Val_Loss",
                            "Val_MAE", "Val_MAPE", "Name"]
        cls.prediction_names = ["Temp Out", "Hi Temp", "Low Temp",
                                "Out Hum", "Wind Speed", "Hi Speed", "Bar  ",
                                "Rain", "Solar Rad.", "Hi Solar Rad. ",
                                "In Temp", "In Hum", "Soils 1 Moist.",
                                "Leaf Wet 1", "Leaf Wet Accum"]

        # Dictionary of datasets with the performance of the benchmarks
        min_max = {"convolutional": {"base": pd.read_csv(PATH_RESOURCES
                                                         + "/benchmark/results/benchmark_convolutional.csv",
                                                         engine="c", header=0, names=cls.column_names)},
                   "temporal": {"base": pd.read_csv(PATH_RESOURCES +
                                                    "/benchmark/results/benchmark_temporal.csv",
                                                    engine="c", header=0, names=cls.column_names)},
                   "indiv_conv": {"base": pd.read_csv(PATH_RESOURCES
                                                      + "/benchmark/results/benchmark_prediction_convolutional.csv",
                                                      engine="c", header=0, names=cls.column_names)},
                   "indiv_temp": {"base": pd.read_csv(PATH_RESOURCES
                                                      + "/benchmark/results/benchmark_prediction_temporal.csv",
                                                      engine="c", header=0, names=cls.column_names)}
                   }

        # Dictionary of the datasets with the predidctions from the benchmarks
        predictions = {"labels": {"base": pd.read_csv(PATH_RESOURCES
                                                      + "/benchmark/database/dataset_hour.csv",
                                                      engine="c")},
                       "prediction conv": {"base": pd.read_csv(PATH_RESOURCES
                                                               +
                                                               "/benchmark/results/prediction/prediction_convolutional.csv",
                                                               engine="c")},
                       "prediction temp": {"base": pd.read_csv(PATH_RESOURCES
                                                               + "/benchmark/results/prediction/prediction_temporal.csv",
                                                               engine="c")}
                       }

        # Pre-process each DataFrames into a list of DataFrames of values to graph
        for key, value in min_max.items():
            # Drops the previous index
            value["base"].drop("Number", axis=1, inplace=True)

            # Retrieves the unique test names in the dataset
            # Assign an id to a test name. Use for colors and markers
            names = [name for name in value["base"]["Name"].drop_duplicates()]

            # Obtains the list of columns to graph with numeric value
            # Removes the last columns ("Name")
            columns = value["base"].columns.tolist()[:-1]

            # DataFrames are in order: 1-min, 2-max, 3-avg, 4-last
            min_max[key]["min"] = pd.DataFrame()
            min_max[key]["max"] = pd.DataFrame()
            min_max[key]["avg"] = pd.DataFrame()
            min_max[key]["last"] = pd.DataFrame()

            # Obtains the values to plot from each test
            # There is a DataFrame for each measurement
            for name in names:
                # Gets the data from a sub-test
                data = value["base"].loc[value["base"]["Name"] == name]

                # Checks if a single value has NaN
                if data.isna().any().any():
                    data = data.dropna(axis=0)
                    # If the data was all NaNs, don't add ir
                    if data.empty:
                        continue

                # 1. Obtains the operation only from the numeric columns (that's why se use "columns")
                # The columns generated by the operation will only be the ones in "columns" and is a pd.Series
                # 2. Adds the name of the sub-test in the Series as a new row. Must be added as as pd.Series
                # 3. Converts the pd.Series with the name into a pd.DataFrame
                # 4. Transpose the Frame and append to the rest of the DataFrame's data
                min_max[key]["min"] = min_max[key]["min"].append(data[columns]
                                                                 .min()
                                                                 .append(pd.Series({"Name": name}))
                                                                 .to_frame()
                                                                 .transpose(), ignore_index=True)
                min_max[key]["avg"] = min_max[key]["avg"].append(data[columns]
                                                                 .mean()
                                                                 .append(pd.Series({"Name": name}))
                                                                 .to_frame()
                                                                 .transpose(), ignore_index=True)
                min_max[key]["max"] = min_max[key]["max"].append(data[columns]
                                                                 .max()
                                                                 .append(pd.Series({"Name": name}))
                                                                 .to_frame()
                                                                 .transpose(), ignore_index=True)
                min_max[key]["last"] = min_max[key]["last"].append(data[columns]
                                                                   .iloc[-1]
                                                                   .append(pd.Series({"Name": name}))
                                                                   .to_frame()
                                                                   .transpose(), ignore_index=True)

        # Stores only the last 168 labels as the ground truth
        predictions["labels"]["base"].drop(["day sin", "day cos", "year sin", "year cos"],
                                           axis=1, inplace=True)
        predictions["labels"]["base"] = predictions["labels"]["base"].tail(168)
        predictions["labels"]["base"].reset_index(drop=True, inplace=True)

        # Divide predictions into a group and individual prediction
        # The group predictions will always be the first 168 rows
        for key, value in predictions.items():
            predictions[key]["group"] = value["base"].head(168)
            predictions[key]["indiv"] = value["base"].tail(-168)

        cls.min_max = min_max
        cls.predictions = predictions

    def test_dataset_convolutional(self):
        # Makes a graph of all the values from the convolutional benchmarks
        cv.graph_epochs(self.min_max["convolutional"]["base"],
                        PATH_RESOURCES + "/images/cross-validation",
                        "convolutional",
                        100)
        # Makes a bar graph from the convolutional benchmarks
        keys = ["min", "max", "avg", "last"]
        cv.graph_performance({key: self.min_max["convolutional"][key] for key in keys},
                             self.column_names[1:-1],
                             PATH_RESOURCES + "/images/cross-validation",
                             "convolutional")
        self.assertTrue(True)

    def test_dataset_temporal(self):
        # Makes a graph of all the values from the convolutional benchmarks
        cv.graph_epochs(self.min_max["temporal"]["base"],
                        PATH_RESOURCES + "/images/cross-validation",
                        "temporal",
                        100)
        # Makes a bar graph from the convolutional benchmarks
        keys = ["min", "max", "avg", "last"]
        cv.graph_performance({key: self.min_max["temporal"][key] for key in keys},
                             self.column_names[1:-1],
                             PATH_RESOURCES + "/images/cross-validation",
                             "temporal")
        self.assertTrue(True)

    def test_dataset_indiv_convolutional(self):
        # Makes a graph of all the values from the convolutional benchmarks
        cv.graph_epochs(self.min_max["indiv_conv"]["base"],
                        PATH_RESOURCES + "/images/cross-validation",
                        "indiv_conv",
                        100)
        # Makes a bar graph from the convolutional benchmarks
        keys = ["min", "max", "avg", "last"]
        cv.graph_performance({key: self.min_max["indiv_conv"][key] for key in keys},
                             self.column_names[1:-1],
                             PATH_RESOURCES + "/images/cross-validation",
                             "indiv_conv")

        self.assertTrue(True)

    def test_dataset_indiv_temporal(self):
        # Makes a graph of all the values from the convolutional benchmarks
        cv.graph_epochs(self.min_max["indiv_temp"]["base"],
                        PATH_RESOURCES + "/images/cross-validation",
                        "indiv_temp",
                        100)
        # Makes a bar graph from the convolutional benchmarks
        keys = ["min", "max", "avg", "last"]
        cv.graph_performance({key: self.min_max["indiv_temp"][key] for key in keys},
                             self.column_names[1:-1],
                             PATH_RESOURCES + "/images/cross-validation",
                             "indiv_temp")

        self.assertTrue(True)

    def test_prediction_conv(self):
        # Makes a bar graph from the convolutional benchmarks
        datasets = {"labels": self.predictions["labels"]["base"],
                    "group": self.predictions["prediction conv"]["group"],
                    "indiv": self.predictions["prediction conv"]["indiv"]}
        cv.graph_predictions(datasets,
                             self.prediction_names,
                             PATH_RESOURCES + "/images/cross-validation",
                             "prediction_conv")
        self.assertTrue(True)

    def test_prediction_temp(self):
        # Makes a bar graph from the convolutional benchmarks
        datasets = {"labels": self.predictions["labels"]["base"],
                    "group": self.predictions["prediction temp"]["group"],
                    "indiv": self.predictions["prediction temp"]["indiv"]}
        cv.graph_predictions(datasets,
                             self.prediction_names,
                             PATH_RESOURCES + "/images/cross-validation",
                             "prediction_temp")
        self.assertTrue(True)


if __name__ == '__main__':
    unittest.main()
